# -*- coding: utf-8 -*-
"""Covid19_Business_CaseStudy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tqxwJ9HN0JibVWcEMS6zDTbGopjsrBpD

# **COVID-19 Business CaseStudy**

**Data Description:**

The COVID-19 pandemic, caused by the SARS-CoV-2 virus, emerged in late 2019 and rapidly spread globally, leading to significant health, economic, and social impacts. This unprecedented health crisis highlighted the crucial role of data analysis in managing such pandemics. By meticulously tracking and analyzing data on confirmed cases, recoveries, and deaths, policymakers and health professionals can make informed decisions to control the spread of the virus and allocate resources effectively.

**DataSet Info:**

This document describes three key datasets used in a case study on the COVID-19 pandemic:

* **Confirmed Cases Dataset:** Daily cumulative confirmed COVID-19 cases per country/region (Jan 22, 2020 - May 29, 2021; over 270 geographic entries).

* **Deaths Dataset:** Daily cumulative deaths attributed to COVID-19, similar structure to confirmed cases.

* **Recovered Cases Dataset:** Daily cumulative recovered individuals from COVID-19.

All datasets include columns for Province/State, Country/Region, geographic coordinates (Lat, Long), and daily cumulative totals by date.

**Data Analysis Problem:** The data needs to be cleaned, transformed, and analyzed to extract meaningful insights.

**1. Data Load**
"""

# imported the all the neccessary library for analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn

# To read csv file data using pandas read_csv function
confirmed = pd.read_csv('/content/covid_19_confirmed_v1_lyst1747728690432.csv')

# Fetch the top 5 rows from dataset
confirmed.head()

# To read csv file data using pandas read_csv function
deaths = pd.read_csv('/content/covid_19_deaths_v1_lyst1747728711771.csv')

# Fetch the top 5 rows from dataset
deaths.head()

# Picking the zero'th index row data and assigning to the columns
deaths.columns = deaths.iloc[0]

# Fetching the data from 1st row till end and resetting or dropping the index and to begin with zero
deaths = deaths[1:].reset_index(drop=True)

"""The dataset of deaths did not have proper headers, so we added headers to ensure accurate data analysis.

"""

# Fetch the top 5 rows from dataset
deaths.head()

# To read csv file data using pandas read_csv function
recovered = pd.read_csv('/content/covid_19_recovered_v1_lyst1747728719904.csv')

# Fetch the top 5 rows from dataset
recovered.head()

# Picking the zero'th index row data and assigning to the columns
recovered.columns = recovered.iloc[0]

# Fetching the data from 1st row till end and resetting or dropping the index and to begin with zero
recovered = recovered[1:].reset_index(drop=True)

"""The dataset of recovered did not have proper headers, so we added headers to ensure accurate data analysis.

"""

# Fetch the top 5 rows from dataset
recovered.head()

"""**2. Data Exploration:**

Q2.1: After loading the datasets, what is the structure of each dataset in terms of rows, columns, and data types?

Q2.2: Generate plots of confirmed cases over time for the top countries.

Q2.3: Generate plots of confirmed cases over time for China.

**After loading the datasets, what is the structure of each dataset in terms of rows, columns, and data types?**
"""

# Displayed the shape of the dataframe (Number of rows, Number of columns)
confirmed.shape

# Displayed information about the data, including column names, data types, and non-null counts
confirmed.info()

# Provided summary statistics such as mean, median, min, max, and standard deviation for the DataFrame
confirmed.describe()

# Provided information about the data types of each column in the dataset
confirmed.dtypes

# Identified and counted missing or blank values for every column in the dataset
confirmed.isnull().sum()

# Identified and counted missing or blank values for particular column in the dataset
confirmed['4/20/20'].isnull().sum()

unique_values = {} # initialized Dictionary
# Iterating the number of column name
for column in confirmed.columns:
  # Stored the unique values of each column in a dictionary called unique_values as a value and column_name as key
 unique_values[column] = confirmed[column].unique()


# unique_values dictionary key and value are displayed
for column, values in unique_values.items():
 print(f'Unique values in {column}: {values}')

# Displayed the shape of the dataframe (Number of rows, Number of columns)
deaths.shape

# Displayed information about the data, including column names, data types, and non-null counts
deaths.info()

# Provided information about the data types of each column in the dataset
deaths.dtypes

"""Displayed the each columns unique values with column name as key"""

unique_values = {}
for column in deaths.columns:
 unique_values[column] = deaths[column].unique()


for column, values in unique_values.items():
 print(f'Unique values in {column}: {values}')

#Displayed statistical summary of the DataFrame
deaths.describe()

"""Used the astype() function to convert the data type of a column to a specified type"""

deaths['Lat'] = deaths['Lat'].astype('float')
deaths['Long'] = deaths['Long'].astype('float')

"""Fetching the data from column of 4th position from dataframe till the end of dataframe"""

deaths_columns = deaths.columns[4:]
deaths_columns

deaths.isnull().sum()

"""Columns without missing values are converted to integers, while those with missing values are stored in non_converted_column for later imputation"""

non_converted_column = []
for col in deaths_columns:
  if deaths[col].isnull().any() == True:
    non_converted_column.append(col)
  else:
    deaths[col] = deaths[col].astype('int')

non_converted_column # list having only one column which contains null values in the data

deaths.dtypes

recovered.shape

recovered.info()

recovered.dtypes

recovered.isnull().sum()

unique_values = {}
for column in recovered.columns:
 unique_values[column] = recovered[column].unique()


for column, values in unique_values.items():
 print(f'Unique values in {column}: {values}')

recovered['Lat'] = recovered['Lat'].astype('float')
recovered['Long'] = recovered['Long'].astype('float')

recovered_columns = recovered.columns[4:]
recovered_columns

non_converted_column_recovered = []
for col in recovered_columns:
  if recovered[col].isnull().any() == True:
    non_converted_column_recovered.append(col)
  else:
    recovered[col] = recovered[col].astype('int')

non_converted_column_recovered

"""**Question 3: Handling Missing Data**

**Q3.1:** Identify these missing values and replace them using a suitable imputation method, such as forward filling, for time-series data.

Filtered non-null rows from the confirmed cases dataset, grouped the data by Province/State, reset the index, converted it into a proper DataFrame, and sorted it in ascending order by Province/State
"""

confirmed[confirmed['Province/State'].isnull() == False]['Province/State'].value_counts().reset_index().sort_values(by='Province/State')

"""Applied forward fill (ffill) imputation to columns containing null values"""

confirmed['Lat'].fillna(method='ffill',inplace=True)
confirmed['Long'].fillna(method='ffill',inplace=True)

"""Applied forward fill (ffill) imputation to columns containing null values"""

deaths['Lat'].fillna(method='ffill', inplace=True)
deaths['Long'].fillna(method='ffill', inplace=True)

deaths[non_converted_column]

deaths['4/20/20'].fillna(method='ffill', inplace=True)

deaths[non_converted_column].isnull().sum()

recovered['4/20/20'].fillna(method='ffill', inplace=True)

recovered['Lat'].fillna(method='ffill', inplace=True)
recovered['Long'].fillna(method='ffill', inplace=True)

recovered[non_converted_column_recovered].isnull().sum()

recovered[non_converted_column_recovered] = recovered[non_converted_column_recovered].astype('int')

deaths[non_converted_column] = deaths[non_converted_column].astype('int')

"""**Question 4: Data Cleaning and Preparation**

Q4.1: Replace blank values in the province column with "All Provinces."
"""

confirmed['Province/State'] = confirmed['Province/State'].fillna("All Provinces")

deaths['Province/State'] = deaths['Province/State'].fillna("All Provinces")

recovered['Province/State'] = recovered['Province/State'].fillna("All Provinces")

recovered.isnull().sum()

"""**Question 6: Data Transformation**

Q6.1: Transform the 'deaths' dataset from wide format (where each column represents a date) to long format (where each row represents a single date), ensuring that the date column is in datetime format. How would this transformation be executed?

Converted the data from wide format to long format using the melt() function for all date columns
"""

long_form_confirmed = confirmed.melt(id_vars=['Province/State','Country/Region','Lat','Long'], var_name='Date_Confirmed')

"""To convert the date type column to datetime datatype pd.to_datetime() function be used"""

long_form_confirmed['Date_Confirmed'] = pd.to_datetime(long_form_confirmed['Date_Confirmed'])

long_form_confirmed

long_form_death = deaths.melt(id_vars=['Province/State','Country/Region','Lat','Long'], var_name='Death_Date')

long_form_death['Death_Date'] = pd.to_datetime(long_form_death['Death_Date'])

long_form_death

long_form_recovered = recovered.melt(id_vars=['Province/State','Country/Region','Lat','Long'], var_name='Recovered_Date')

long_form_recovered['Recovered_Date'] = pd.to_datetime(long_form_recovered['Recovered_Date'])

long_form_recovered

"""Q6.2: What is the total number of deaths reported per country up to the current date?

Drop the duplicate rows from dataframe drop_duplicates() used
"""

long_form_death.drop_duplicates()

"""Grouped the data by the Country/Region column and calculated the total number of deaths for each country or region"""

long_form_death.groupby('Country/Region').agg({'value':'sum'}).reset_index()

"""Q6.3: What are the top 5 countries with the highest average daily deaths?

The United States ranks highest in average death rate, with Brazil, India, Mexico, and Italy also among the top five countries
"""

long_form_death.groupby('Country/Region')['value'].mean().reset_index().sort_values(by='value', ascending=False).head()

"""Q6.4: How have the total deaths evolved over time in the United States?"""

US_Data = long_form_death[long_form_death['Country/Region']=='US'] # fetched the US country data from the death cases dataframe

US_Data = US_Data.groupby('Death_Date')['value'].sum().reset_index()

"""During the specified time period, the number of death cases increased day by day"""

plt.figure(figsize=(9,6))
plt.plot(US_Data['Death_Date'], US_Data['value'])
plt.xlabel("Date")
plt.ylabel('Total number of death per day')
plt.xticks(rotation=45)
plt.show()

"""Q2.2: Generate plots of confirmed cases over time for the top countries.

confirmed cases over time calculated for top countries
"""

top_country_confirm_cases = long_form_confirmed.groupby('Country/Region')['value'].sum().reset_index().sort_values(by='value', ascending=False).head()

top_country_confirm_cases

top_country_data = long_form_confirmed[long_form_confirmed['Country/Region'].isin(top_country_confirm_cases['Country/Region'])]

"""The specified period was the peak of the COVID-19 pandemic, during which every country experienced a surge in confirmed cases"""

plt.figure(figsize=(9,6))
for country in top_country_confirm_cases['Country/Region']:
  country_data = top_country_data[top_country_data['Country/Region']==country]
  country_data_confirm_cases = country_data.groupby('Date_Confirmed')['value'].sum().reset_index()
  plt.plot(country_data_confirm_cases['Date_Confirmed'], country_data_confirm_cases['value'])
  plt.xlabel("Date")
  plt.ylabel('Total number of confirmed cases per day')
  plt.title(f"{country} Country wise confirmed cases over time")
  plt.xticks(rotation=45)
  plt.show()
  print()

"""Q2.3: Generate plots of confirmed cases over time for China."""

china_confirm_cases = long_form_confirmed[long_form_confirmed['Country/Region']=='China']

china_final_confirm_cases = china_confirm_cases.groupby('Date_Confirmed')['value'].sum().reset_index()

"""China experienced a steady rise in cases for a certain period, followed by a sharp increase"""

plt.figure(figsize=(9,6))
plt.plot(china_final_confirm_cases['Date_Confirmed'], china_final_confirm_cases['value'])
plt.xlabel("Date")
plt.ylabel('Total number of confirm cases per day')
plt.title("China confirmed cases over time")
plt.xticks(rotation=45)
plt.show()

"""**Question 5: Independent Dataset Analysis**

Q5.1: Analyze the peak number of daily new cases in Germany, France, and Italy. Which country experienced the highest single-day surge, and when did it occur?

Used the diff() function to compute the day-to-day increase in COVID-19 cases for Germany

diff() - The function is used to calculate the difference between consecutive rows in a column of a DataFrame or Series.
"""

Daily_cases_Germany_number = long_form_confirmed[long_form_confirmed['Country/Region']=='Germany'].copy()
Daily_cases_Germany_number['Daily_cases_Germany'] = long_form_confirmed[long_form_confirmed['Country/Region']=='Germany'].groupby('Country/Region')['value'].diff().fillna(0)

"""highest one fetch from germany daily confirm cases"""

Daily_cases_Germany_number = Daily_cases_Germany_number.sort_values(by='Daily_cases_Germany', ascending=False)[['Country/Region','Date_Confirmed', 'Daily_cases_Germany']].head(1)

Daily_cases_Germany_number

"""Used the diff() function to compute the day-to-day increase in COVID-19 cases for France"""

Daily_cases_France_number = long_form_confirmed[long_form_confirmed['Country/Region']=='France'].copy()
Daily_cases_France_number['Daily_cases_France'] = long_form_confirmed[long_form_confirmed['Country/Region']=='France'].groupby('Country/Region')['value'].diff().fillna(0)

Daily_cases_France_number = Daily_cases_France_number.sort_values(by='Daily_cases_France', ascending=False)[['Country/Region','Date_Confirmed', 'Daily_cases_France']].head(1)

Daily_cases_France_number

"""Used the diff() function to compute the day-to-day increase in COVID-19 cases for Italy"""

Daily_cases_Italy_number = long_form_confirmed[long_form_confirmed['Country/Region']=='Italy'].copy()
Daily_cases_Italy_number['Daily_cases_Italy'] = long_form_confirmed[long_form_confirmed['Country/Region']=='Italy'].groupby('Country/Region')['value'].diff().fillna(0)

Daily_cases_Italy_number = Daily_cases_Italy_number.sort_values(by='Daily_cases_Italy', ascending=False)[['Country/Region','Date_Confirmed', 'Daily_cases_Italy']].head(1)

Daily_cases_Italy_number

"""Used the diff() function to compute the day-to-day increase in COVID-19 cases for Germany"""

country_wise_daily_high_cases = pd.concat([Daily_cases_Germany_number,Daily_cases_France_number,Daily_cases_Italy_number])

final_data = country_wise_daily_high_cases.melt(id_vars=['Country/Region', 'Date_Confirmed'],var_name='Daily_Cases')

"""Dropped all nan value and reset the index of dataframe"""

final_data = final_data.dropna()
final_data = final_data.reset_index(drop=True)

"""Highest confirmed cases occurred on France country"""

final_data

final_data.sort_values(by='value', ascending=False).head(1)

"""Q5.2: Compare the recovery rates (recoveries/confirmed cases) between Canada and Australia as of December 31, 2020. Which country showed better management of the pandemic according to this metric?

Recovery cases having data up to 2020-12-31 for canada and australia country
"""

recovered_country = long_form_recovered[long_form_recovered['Country/Region'].isin(['Canada', 'Australia'])]
recovered_country = recovered_country[recovered_country['Recovered_Date']<='2020-12-31']

confirmed_country = long_form_confirmed[long_form_confirmed['Country/Region'].isin(['Canada', 'Australia'])]
confirmed_country = confirmed_country[confirmed_country['Date_Confirmed']<='2020-12-31']

"""Total recovered cases for canada country"""

total_recovered_canada = recovered_country[recovered_country['Country/Region']=='Canada']['value'].sum()

"""Total recovered cases for australia country"""

total_recovered_australia = recovered_country[recovered_country['Country/Region']=='Australia']['value'].sum()

"""Total confirmed cases for canada country"""

total_confirmed_canada = confirmed_country[confirmed_country['Country/Region']=='Canada']['value'].sum()

"""Total recovered cases for australia country"""

total_confirmed_australia = confirmed_country[confirmed_country['Country/Region']=='Australia']['value'].sum()

"""The recovery rate was calculated as total recovered cases divided by total confirmed cases for each country. Canada had a higher recovery rate compared to Australia"""

Rate_of_recovery_canada = (total_recovered_canada/total_confirmed_canada)*100
Rate_of_recovery_canada

Rate_of_recovery_australia = (total_recovered_australia/total_confirmed_australia)*100
Rate_of_recovery_australia

"""Q5.3: What is the distribution of death rates (deaths/confirmed cases) among provinces in Canada? Identify the province with the highest and lowest death rate as of the latest data point."""

province_death = long_form_death[long_form_death['Country/Region']=='Canada'].sort_values(by=['Province/State','Death_Date'])

province_death = province_death.groupby('Province/State').tail(1).reset_index(drop=True)[['Province/State','Country/Region','value']]

province_confirm = long_form_confirmed[long_form_confirmed['Country/Region']=='Canada'].sort_values(by=['Province/State','Date_Confirmed'])

province_confirm = province_confirm.groupby('Province/State').tail(1).reset_index(drop=True)[['Province/State','Country/Region','value']]

Total_province = province_death.merge(province_confirm, on='Province/State', suffixes=('_death', '_confirm'))

Total_province['death_rate']=(Total_province['value_death']/Total_province['value_confirm'])*100

"""Highest death rate is of **Diamond Princess** Province/State and lowest death rate is **Grand Princess**"""

Total_province = Total_province[['Province/State', 'value_death', 'value_confirm', 'death_rate']]
Total_province = Total_province.sort_values(by='death_rate').reset_index(drop=True)
Total_province

"""**Question 7: Data Merging**

Q7.1: How would you merge the transformed datasets of confirmed cases, deaths, and recoveries on the 'Country/Region' and 'Date' columns to create a comprehensive view of the pandemic's impact?
"""

two_dataset_merge =  long_form_confirmed.merge(long_form_death, left_on=['Country/Region','Date_Confirmed'], right_on=['Country/Region','Death_Date'], how='inner', suffixes=('_confirmed','_death'))

Final_total_merge_data = two_dataset_merge.merge(long_form_recovered, left_on=['Country/Region', 'Death_Date'], right_on=['Country/Region','Recovered_Date'])

"""All the three dataset are merged using merge function of dataframe"""

Final_total_merge_data.head()

"""Q7.2: Analyze the monthly sum of confirmed cases, deaths, and recoveries for countries to understand the progression of the pandemic.[From the merged dataset]"""

Final_total_merge_data['Month'] = Final_total_merge_data['Date_Confirmed'].dt.month

Final_total_merge_data['Year'] = Final_total_merge_data['Date_Confirmed'].dt.year

analyse_data =  Final_total_merge_data[['Country/Region', 'value_confirmed', 'value_death', 'value', 'Month']]

analyse_data_confirmed_cases =  analyse_data.groupby(['Country/Region','Month'])['value_confirmed'].sum().reset_index()

"""Monthly cases are displayed as per the country wise. Ten country's data are displayed at a time on one graph. Mostly found that first quarter has the highest confirmed cases in country"""

countries = analyse_data_confirmed_cases['Country/Region'].unique()

for i in range(0, len(countries), 10):
    batch_countries = countries[i:i+10]

    data = analyse_data_confirmed_cases[analyse_data_confirmed_cases['Country/Region'].isin(batch_countries)]

    plt.figure(figsize=(12, 6))
    sn.barplot(data=data,x='Country/Region',y='value_confirmed',hue='Month')

    plt.xlabel("Country/Region")
    plt.ylabel("Total Confirmed Cases")
    plt.title("Confirmed Cases per Month (10 Countries at a Time)")
    plt.xticks(rotation=45)
    plt.legend(title='Month')
    plt.tight_layout()
    plt.show()

analyse_data_death_cases =  analyse_data.groupby(['Country/Region','Month'])['value_death'].sum().reset_index()

countries = analyse_data_death_cases['Country/Region'].unique()

# Plot in batches of 10 countries
for i in range(0, len(countries), 10):
    batch_countries = countries[i:i+10]

    data = analyse_data_death_cases[analyse_data_death_cases['Country/Region'].isin(batch_countries)]

    plt.figure(figsize=(12, 6))
    sn.barplot(data=data,x='Country/Region',y='value_death',hue='Month')

    plt.xlabel("Country/Region")
    plt.ylabel("Total death Cases")
    plt.title("death Cases per Month (10 Countries at a Time)")
    plt.xticks(rotation=45)
    plt.legend(title='Month')
    plt.tight_layout()
    plt.show()

analyse_data_recovery_cases =  analyse_data.groupby(['Country/Region','Month'])['value'].sum().reset_index()

countries = analyse_data_recovery_cases['Country/Region'].unique()

# Plot in batches of 10 countries
for i in range(0, len(countries), 10):
    batch_countries = countries[i:i+10]

    data = analyse_data_recovery_cases[analyse_data_recovery_cases['Country/Region'].isin(batch_countries)]

    plt.figure(figsize=(12, 6))
    sn.barplot(data=data,x='Country/Region',y='value',hue='Month')

    plt.xlabel("Country/Region")
    plt.ylabel("Total recovered Cases")
    plt.title("Recovered Cases per Month (10 Countries at a Time)")
    plt.xticks(rotation=45)
    plt.legend(title='Month')
    plt.tight_layout()
    plt.show()

"""Q7.3: Redo the analysis in Question 7.2 for the United States, Italy, and Brazil."""

three_country_data = Final_total_merge_data[Final_total_merge_data['Country/Region'].isin(['US', 'Italy', 'Brazil'])][['Country/Region', 'value_confirmed', 'value_death', 'value', 'Month']]

three_country_data_confirm =  three_country_data.groupby(['Country/Region','Month'])['value_confirmed'].sum().reset_index()

countries = three_country_data['Country/Region'].unique()

# Plot in batches of 10 countries
for i in range(0, len(countries)):
    batch_countries = countries[i]

    data = three_country_data_confirm[three_country_data_confirm['Country/Region'] == batch_countries]

    plt.figure(figsize=(12, 6))
    sn.barplot(data=data,x='Country/Region',y='value_confirmed',hue='Month')

    plt.xlabel("Country/Region")
    plt.ylabel("Total Confirm Cases")
    plt.title("Confirm Cases per Month (10 Countries at a Time)")
    plt.xticks(rotation=45)
    plt.legend(title='Month')
    plt.tight_layout()
    plt.show()

three_country_data_death =  three_country_data.groupby(['Country/Region','Month'])['value_death'].sum().reset_index()

countries = three_country_data['Country/Region'].unique()

# Plot in batches of 10 countries
for i in range(0, len(countries)):
    batch_countries = countries[i]

    data = three_country_data_death[three_country_data_death['Country/Region'] == batch_countries]

    plt.figure(figsize=(12, 6))
    sn.barplot(data=data,x='Country/Region',y='value_death',hue='Month')

    plt.xlabel("Country/Region")
    plt.ylabel("Total death Cases")
    plt.title("Death Cases per Month (10 Countries at a Time)")
    plt.xticks(rotation=45)
    plt.legend(title='Month')
    plt.tight_layout()
    plt.show()

three_country_data_recovery =  three_country_data.groupby(['Country/Region','Month'])['value'].sum().reset_index()

countries = three_country_data['Country/Region'].unique()

# Plot in batches of 10 countries
for i in range(0, len(countries)):
    batch_countries = countries[i]

    data = three_country_data_recovery[three_country_data_recovery['Country/Region'] == batch_countries]

    plt.figure(figsize=(12, 6))
    sn.barplot(data=data,x='Country/Region',y='value',hue='Month')

    plt.xlabel("Country/Region")
    plt.ylabel("Total recovery Cases")
    plt.title("Recovery Cases per Month (10 Countries at a Time)")
    plt.xticks(rotation=45)
    plt.legend(title='Month')
    plt.tight_layout()
    plt.show()

"""**Question 8: Combined Data Analysis**

Q8.1: For the combined dataset, identify the three countries with the highest average death rates (deaths/confirmed cases) throughout 2020. What might this indicate about the pandemic's impact in these countries?
"""

data_merge=Final_total_merge_data[Final_total_merge_data['Year']==2020][['Country/Region','value_confirmed','value_death','Year']]

data_merge['death_rate'] = (data_merge['value_death']/data_merge['value_confirmed']).fillna(0)

data_merge_death = data_merge.groupby('Country/Region')['death_rate'].mean().reset_index()

"""Highest country of avearge death rate is of Australia, France and Canada"""

data_merge_death.sort_values(by='death_rate', ascending=False).head(3)

"""Q8.2: Using the merged dataset, compare the total number of recoveries to the total number of deaths in South Africa. What can this tell us about the outcomes of COVID-19 cases in the country?"""

south_africa_data = Final_total_merge_data[Final_total_merge_data['Country/Region']=='South Africa']

total_death_cases =  np.sum(south_africa_data['value_death'])
total_recovery_cases = np.sum(south_africa_data['value'])

total_death_cases

"""As comparing recovery cases are more as compare to the death cases in south africa"""

total_recovery_cases

"""Q8.3: Analyze the ratio of recoveries to confirmed cases for the United States monthly from March 2020 to May 2021. Which month experienced the highest recovery ratio, and what could be the potential reasons?"""

US_Merge_Data = Final_total_merge_data[
    (Final_total_merge_data['Country/Region'] == 'US') &
    (Final_total_merge_data['Date_Confirmed'] >= '2020-03-01') &
    (Final_total_merge_data['Date_Confirmed'] <= '2021-05-31')]

US_Merge_Data = US_Merge_Data.groupby(['Month','Year']).agg({'value_confirmed':'sum','value':'sum'}).reset_index().sort_values(by=['Year', 'Month'])

US_Merge_Data['Recovery_Ratio'] = (US_Merge_Data['value']/US_Merge_Data['value_confirmed'])*100

US_Merge_Data.sort_values(by='Recovery_Ratio', ascending=False)

"""The United States recorded its highest recovery rate in October 2020"""

US_Merge_Data.sort_values(by='Recovery_Ratio', ascending=False).head(1)